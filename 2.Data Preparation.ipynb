{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/sherman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "FILE_NAME = \"covid19_articles_20201231.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/\" + FILE_NAME)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE THIS LINE FOR PROD ##\n",
    "## USED FOR TESTING TO MAKE THINGS FASTER ##\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, stratify=y)\n",
    "X = X_test[[\"content\"]]\n",
    "y = y_test\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "## END ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stemm=False, lemm=False, stopwords=None):\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text.lower().strip())\n",
    "    if stopwords:\n",
    "        text = [x for x in text.split() if x not in stopwords]\n",
    "\n",
    "    if stemm:\n",
    "        stemmer = PorterStemmer()\n",
    "        text = [stemmer.stem(x) for x in text]\n",
    "\n",
    "    if lemm:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = [lemmatizer.lemmatize(x) for x in text]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "STOP_WORDS = set(nltk.corpus.stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"clean\"] = X[\"content\"].apply(\n",
    "    lambda x: clean_text(x, lemm=True, stopwords=STOP_WORDS)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18453, 10000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    max_features=10000, ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "tfidf_corpus = X[\"clean\"]\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(tfidf_corpus)\n",
    "tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "print(tfidf_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce dimensionality by using Chi-Square test\n",
    "## generate most significant words\n",
    "tfidf_X_names = tfidf_vectorizer.get_feature_names_out()\n",
    "p_value_limit = 0.95\n",
    "\n",
    "_ = pd.DataFrame()\n",
    "for category in np.unique(y):\n",
    "    chi_square, p = chi2(tfidf_vectors, y == category)\n",
    "    _ = pd.concat(\n",
    "        [_, pd.DataFrame({\"feature\": tfidf_X_names, \"score\": 1 - p, \"y\": category})]\n",
    "    )\n",
    "    _ = _.sort_values([\"y\", \"score\"], ascending=[True, False])\n",
    "    _ = _[_[\"score\"] > p_value_limit]\n",
    "\n",
    "tfidf_X_names = _[\"feature\"].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18453, 2457)\n"
     ]
    }
   ],
   "source": [
    "## regenerate vectors\n",
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    vocabulary=tfidf_X_names\n",
    ")\n",
    "\n",
    "tfidf_corpus = X[\"clean\"]\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(tfidf_corpus)\n",
    "tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "print(tfidf_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18453, 2457)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save it for future use\n",
    "scipy.sparse.save_npz(\"processed_data/tfidf_sparse_matrix\", tfidf_vectors)\n",
    "tfidf_vectors = scipy.sparse.load_npz(\"processed_data/tfidf_sparse_matrix.npz\")\n",
    "tfidf_vectors.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = gensim_api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_corpus = X[\"clean\"]\n",
    "\n",
    "# unigrams\n",
    "_ = []\n",
    "for row in we_corpus:\n",
    "    _.append(row.split())\n",
    "we_corpus = _\n",
    "\n",
    "bigram_phraser = Phraser(Phrases(we_corpus, delimiter=\" \", min_count=5, threshold=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insertion certified', 'viral vector', 'expressly disclaims', 'upper tier', 'ability satisfy', 'root cause', 'rp panel', 'mexico enduse', 'yoy yoy', 'coor share']\n",
      "['insertion certified', 'viral vector', 'expressly disclaims', 'reopened economy', 'unfavorable change', 'ability satisfy', 'root cause', 'upper tier', 'mexico enduse', 'yoy yoy']\n"
     ]
    }
   ],
   "source": [
    "b_set = set()\n",
    "for row in we_corpus:\n",
    "    for word in bigram_phraser[row]:\n",
    "        if \" \" in word:\n",
    "            b_set.add(word)\n",
    "print(list(b_set)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
