{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with BERT\n",
    "\n",
    "References:\n",
    "\n",
    "- https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "- https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    "    AutoTokenizer,\n",
    "    BertTokenizerFast,\n",
    "    AutoModel,\n",
    "    DistilBertTokenizerFast,\n",
    ")\n",
    "from keras.layers import Input, GlobalAveragePooling1D, Dense, Dropout\n",
    "from keras.models import Model\n",
    "import regex as re\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/covid19_articles_20201231_reduced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28241</td>\n",
       "      <td>The coronavirus crisis has almost certainly en...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210240</td>\n",
       "      <td>Latest Report Shows a 15.3% Week-Over-Week Dec...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77957</td>\n",
       "      <td>FORESIGHT VCT PLC (Company) Publication of Sup...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207961</td>\n",
       "      <td>Technavio has been monitoring the global mater...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>252956</td>\n",
       "      <td>Outdoor pop-up classes will be held in parks a...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content topic_area\n",
       "0       28241  The coronavirus crisis has almost certainly en...   business\n",
       "1      210240  Latest Report Shows a 15.3% Week-Over-Week Dec...   business\n",
       "2       77957  FORESIGHT VCT PLC (Company) Publication of Sup...   business\n",
       "3      207961  Technavio has been monitoring the global mater...   business\n",
       "4      252956  Outdoor pop-up classes will be held in parks a...   business"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE ME\n",
    "_, X_train, __, y_train = train_test_split(\n",
    "    df[\"content\"],\n",
    "    df[\"topic_area\"],\n",
    "    test_size=0.005,\n",
    "    stratify=df[\"topic_area\"],\n",
    "    random_state=69,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lightweight version of bert\n",
    "## https://huggingface.co/docs/transformers/preprocessing\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BERT_MODEL_NAME, do_lower_case=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "\n",
    "\n",
    "def bert_encode(corpus, max_length):\n",
    "    res = [[], [], []]\n",
    "    for sentence in corpus:\n",
    "        sentence = (\n",
    "            [\"[CLS]\"]\n",
    "            + [\n",
    "                \" \".join(tokenizer.tokenize(x))\n",
    "                for x in (re.sub(r\"[^\\w\\s]+|\\n\", \"\", sentence))\n",
    "                .lower()\n",
    "                .strip()\n",
    "                .split(\" \")\n",
    "                if x.strip() != \"\"\n",
    "            ][: max_length // 4]\n",
    "            + [\"[SEP]\"]\n",
    "        )\n",
    "\n",
    "        tokens = [\n",
    "            inner\n",
    "            for outer in [tokenizer.encode(x)[1:-1] for x in sentence]\n",
    "            for inner in outer\n",
    "        ]\n",
    "\n",
    "        ## padding\n",
    "        tokens = tokens + [0] * max(0, max_length - len(tokens))\n",
    "\n",
    "        mask = []\n",
    "        token_type_id = []\n",
    "        m, s = 1, 0\n",
    "        for t in tokens:\n",
    "            if t == 102:\n",
    "                m, s = 0, 1\n",
    "            mask.append(m)\n",
    "            token_type_id.append(s)\n",
    "        res[0].append(np.array(tokens))\n",
    "        res[1].append(np.array(mask))\n",
    "        res[2].append(np.array(token_type_id))\n",
    "    return np.array([np.array(x) for x in res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = bert_encode(X_train, MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 236, 100)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLengths = [0, 0, 0]\n",
    "for i in range(len(X_train_encoded)):\n",
    "    for j in range(3):\n",
    "        maxLengths[j] = max(maxLengths[j], len(X_train_encoded[j][i]))\n",
    "maxLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_55 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_56 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_57 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_18 (TFBertModel)  TFBaseModelOutputWi  109482240  ['input_55[0][0]',               \n",
      "                                thPoolingAndCrossAt               'input_56[0][0]',               \n",
      "                                tentions(last_hidde               'input_57[0][0]']               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_11 (G  (None, 768)         0           ['tf_bert_model_18[0][0]']       \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dropout_711 (Dropout)          (None, 768)          0           ['global_average_pooling1d_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 64)           49216       ['dropout_711[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 7)            455         ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,531,911\n",
      "Trainable params: 49,671\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "idx = Input((MAX_LENGTH,), dtype=\"int32\")\n",
    "masks = Input((MAX_LENGTH,), dtype=\"int32\")\n",
    "token_type_ids = Input((MAX_LENGTH,), dtype=\"int32\")\n",
    "\n",
    "custom_input = [idx, masks, token_type_ids]\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_output_layer = bert_model(custom_input)\n",
    "\n",
    "layer = GlobalAveragePooling1D()(bert_output_layer[\"last_hidden_state\"])\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(64, activation=\"relu\")(layer)\n",
    "output_layer = Dense(len(np.unique(y_train)), activation=\"softmax\")(layer)\n",
    "\n",
    "model = Model(custom_input, output_layer)\n",
    "\n",
    "for layer in model.layers[:4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_y_mapping = {n: label for n, label in enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v: k for k, v in dic_y_mapping.items()}\n",
    "y_train_dummy = np.array([inverse_dic[y] for y in y_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236,)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 3\n  y sizes: 236\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sherman/Github/IS424 T4/5.BERT Transfer Learning.ipynb Cell 15'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sherman/Github/IS424%20T4/5.BERT%20Transfer%20Learning.ipynb#ch0000018?line=0'>1</a>\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mX_train_encoded, y\u001b[39m=\u001b[39;49my_train_dummy, batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py:1653\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py?line=1648'>1649</a>\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py?line=1649'>1650</a>\u001b[0m       label, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py?line=1650'>1651</a>\u001b[0m                        \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)))\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py?line=1651'>1652</a>\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py?line=1652'>1653</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 3\n  y sizes: 236\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=X_train_encoded, y=y_train_dummy, batch_size=10, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
