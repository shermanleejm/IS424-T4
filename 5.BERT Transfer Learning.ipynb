{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference from Hugging Face  \n",
    "https://huggingface.co/course/chapter3/3?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/covid19_articles_20201231_reduced.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>topic_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28241</td>\n",
       "      <td>The coronavirus crisis has almost certainly en...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210240</td>\n",
       "      <td>Latest Report Shows a 15.3% Week-Over-Week Dec...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77957</td>\n",
       "      <td>FORESIGHT VCT PLC (Company) Publication of Sup...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207961</td>\n",
       "      <td>Technavio has been monitoring the global mater...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>252956</td>\n",
       "      <td>Outdoor pop-up classes will be held in parks a...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content topic_area\n",
       "0       28241  The coronavirus crisis has almost certainly en...   business\n",
       "1      210240  Latest Report Shows a 15.3% Week-Over-Week Dec...   business\n",
       "2       77957  FORESIGHT VCT PLC (Company) Publication of Sup...   business\n",
       "3      207961  Technavio has been monitoring the global mater...   business\n",
       "4      252956  Outdoor pop-up classes will be held in parks a...   business"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/44114463/stratified-sampling-in-pandas\n",
    "def stratified_sample_df(df, col, n_samples, random_state=69):\n",
    "    n = min(n_samples, df[col].value_counts().min())\n",
    "    df_ = df.groupby(col).apply(lambda x: x.sample(n, random_state=random_state))\n",
    "    df_.index = df_.index.droplevel(0)\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stratified_sample_df(df, \"topic_area\", 10)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, truncate=True, max_length=100)\n",
    "sequences = df[\"content\"].tolist()\n",
    "batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample sentence:     - Craft beer, live music and lodging featured in renovation plans that Takuya Shimbo had for an agi...\n",
      "input ids:          [  101  1011  7477  5404  1010  2444  2189  1998 26859  2956  1999 10525\n",
      "  3488  2008 27006 26230  2050 11895 13344  2018  2005  2019 12520  5522\n",
      "  7198  4580  1010  5327  2000  5343  1037 14059  3068  2013 14446  2011\n",
      " 27788 26703  1996  4145  1997 15029 17573  1012  2059  1996  3117 21887\n",
      " 23350  4930  1012  1996  2231  8357  2900  1005  1055  2261  3588  7198\n",
      " 15666  2004  4187  2005  2270 19548  2061  2009  7303  2027  2994  2330\n",
      "  1010  2096  2012  1996  2168  2051 11434  2111  2000  2994  2012  2188\n",
      "  2076  1037  2110  1997  5057  2000  4652  1996  3659  1997  2522 17258\n",
      "  1011  2539  2029  2038]...\n",
      "token type ids:     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]...\n",
      "attention mask:     [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "Sample sentence:    {df[\"content\"].tolist()[0][:100]}...\n",
    "input ids:          {batch[\"input_ids\"][0][:100]}...\n",
    "token type ids:     {batch[\"token_type_ids\"][0][:100]}...\n",
    "attention mask:     {batch[\"attention_mask\"][0][:100]}...\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_area_map = {k: i for k, i in enumerate(df[\"topic_area\"].unique())}\n",
    "reversed_topic_area_map = {v: k for k, v in topic_area_map.items()}\n",
    "labels = tf.convert_to_tensor(\n",
    "    [reversed_topic_area_map[k] for k in df[\"topic_area\"].tolist()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 141s 42s/step - loss: 8.4256 - accuracy: 0.1250 - val_loss: 17.0822 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 122s 40s/step - loss: 6.2511 - accuracy: 0.1786 - val_loss: 17.4661 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 119s 39s/step - loss: 5.9846 - accuracy: 0.1786 - val_loss: 17.4357 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 121s 39s/step - loss: 5.9691 - accuracy: 0.1786 - val_loss: 17.3326 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 121s 40s/step - loss: 6.0142 - accuracy: 0.1786 - val_loss: 17.5044 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=len(df.topic_area.unique())\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    x=batch,\n",
    "    y=labels,\n",
    "    batch_size=20,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 12:04:50.061784: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/bert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/bert/assets\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_FILEPATH = \"saved_models/bert\"\n",
    "model.save(BERT_MODEL_FILEPATH)\n",
    "model = tf.keras.models.load_model(BERT_MODEL_FILEPATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"this is a super test\", \"this is a boring article\"]\n",
    "batch = dict(tokenizer(test, padding=True, truncation=True, return_tensors=\"tf\"))\n",
    "y_pred = model.predict(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.8829320669174194,\n",
       "  1.4541959762573242,\n",
       "  1.1529698371887207,\n",
       "  3.1338181495666504,\n",
       "  2.0458924770355225,\n",
       "  -0.9047433733940125,\n",
       "  -1.2239004373550415],\n",
       " [-0.8821392059326172,\n",
       "  1.4532511234283447,\n",
       "  1.1529271602630615,\n",
       "  3.132197380065918,\n",
       "  2.0450141429901123,\n",
       "  -0.9033188819885254,\n",
       "  -1.2226083278656006]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.to_tuple()[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
